---
title: "homework chapter 9"
author: "Dennis Liu"
date: "July 19th, 2015"
output:
  pdf_document:
    fig_height: 4.3
    fig_width: 8
    keep_tex: yes
  html_document:
    fig_height: 4
    fig_width: 8
geometry: margin=1in
setspace: onehalfspacing
fontsize: 12pt
---


Q1. This problem involves hyperplanes in two dimensions.

(a) Sketch the hyperplane $1 + 3X_1 - X_2 = 0$. Indicate the set of points for which $1 + 3X_1 - X_2 > 0$, as well as the set of points for which $1 + 3X_1 - X_2 < 0$.

(b) On the same plot, sketch the hyperplane $-2 + X_1 + 2X_2 = 0$. Indicate the set of points for which $-2 + X_1 + 2X_2 > 0$, as well as the set of points for which $-2 + X_1 + 2X_2 < 0$.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
x1 = -10:10
x2 = 1 + 3 * x1
plot(x1, x2, type = "l", col = "blue",xlim=c(-12,12))
text(c(-4), c(-20), "Greater than 0", col = "blue")
text(c(4), c(20), "Less than 0", col = "blue")
lines(x1, 1 - x1/2, col = "red")
text(c(2), c(-5), "Less than 0", col = "red")
text(c(-2), c(5), "Greater than 0", col = "red")
```

Q2. We have seen that in $p = 2$ dimensions, a linear boundary takes the form $\beta_0 + \beta_1X_1 + \beta_2X_2 = 0$. We now investigate a non-linear decision boundary.

(a) Sketch the curve $$(1 + X_1)^2 + (2 - X_2)^2 = 4$$

(b) On your sketch, indicate the set of points for which $$(1 + X_1)^2 + (2 - X_2)^2 > 4$$, as well as the set of points for which $$(1 + X_1)^2 + (2 - X_2)^2 \le 4$$.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, 
     xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE)
text(c(-1), c(2), "< 4")
text(c(-4), c(2), "> 4")
text(c(4), c(2), "> 4")
# plot(c(0, -1, 2, 3), c(0, 1, 2, 8), col = c("blue", "red", "blue", "blue"),  type = "p", asp = 1, xlab = "X1", ylab = "X2")
```

(c) Suppose that a classifier assigns an observation to the blue class if $$(1 + X_1)^2 + (2 - X_2)^2 > 4$$, and to the red class otherwise. To what class is the observation $(0,0)$ classified ? $(-1,1)$ ? $(2,2)$ ? $(3,8)$ ?

- all points except $(-1,1)$ will be classified as blue since they are outside of the circle; $(-1,1)$ would be red. 


(d) Argue that while the decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$ and $X_2^2$.

- If we expand the equation of the decision boundary given by part (c), we have $X_1^2 + X_2^2 + 2X_1 - 4X_2 + 1 = 0$ which is consisted of linear terms of $X_1$, $X_1^2$, $X_2$ and $x_2^2$.

\hfill

Q3. Here we explore the maximal margin classifier on a toy data set.


(a) We are given $n = 7$ observations in $p = 2$ dimensions. For each observation there is an associated class label. 
$$\begin{array}{cccc} \hline \mbox{Obs.} &X_1 &X_2 &Y \cr \hline 1 &3 &4 &\mbox{Red} \cr 2 &2 &2 &\mbox{Red} \cr 3 &4 &4 &\mbox{Red} \cr 4 &1 &4 &\mbox{Red} \cr 5 &2 &1 &\mbox{Blue} \cr 6 &4 &3 &\mbox{Blue} \cr 7 &4 &1 &\mbox{Blue} \cr \hline \end{array}$$

Sketch the observations.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
```

(b) Sketch the optimal separating hyperplane and provide the equation for this hyperplane (of the form (9.1)).

By the maximum margin assumption, the optimal separating hyperplane must be between $(2,1)$ and $(2,2)$ and between $(4,3)$ and $(4,4)$. We just take their mid-point and link two dots. 

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1, col=5)
```

(c) Describe the classifiacation rule for the maximal margin classifier. It should be something along the lines of "Classify to Red if $\beta_0 + \beta_1X_1 + \beta_2X_2 > 0$, and classify to Blue otherwise." Provide the values for $\beta_0$, $\beta_1$ and $\beta_2$.

- The classification rule is Red if $X_1 - X_2 -0.5 < 0$, and Blue if $X_1 - X_2 -0.5 > 0$.


(d) On your sketch, indicate the margin for the maximal margin hyperplane.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2, col = "purple")
abline(0, 1, lty = 2, col = "purple")
# The margin is here equal to 1/4
```


(e) Indicate the support vectors for the maximal margin classifier.

- The support vectors are the points alongside the margin. They are $(2,1)$, $(2,2)$, $(4,3)$ and $(4,4)$.


(f) Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.

- The seventh observation is $(4,1)$. Since it is not on the margin (e.g., support vector), the maximal margin hyperplane would not change if it is slightly moved. 


(g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.2, 1)
```

- The hyperplane which equation, $X_1 - X_2 - 0.2 = 0$ is not the optimal separating hyperplane.


(h) Draw an additional observation on the plot so that two classes are no longer separable by a hyperplane.

```{r, echo=F,message=FALSE,comment=NA,warning=FALSE}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(5), c(1), col = c("red"), pch = 13)
```

\hfill

Q5. We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.

(a) Generate a data set with $n = 500$ and $p = 2$, such that the observations belong to two classes with a quadratic decision boundary between them.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
set.seed(3)
library(e1071)
x1 = runif(500) - 0.5 #uniform distribution
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
plot(x1, x2, xlab = "X1", ylab = "X2", col = (4 - y), pch = (3 - y))
```

(b) Plot the observations, colored according to their class labels. Your plot should display $X_1$ on the $x$-axis and $X_2$ on the $y$-axis.

- As shown above, it is an ellipse equation. The outward side is classified as 1 (left and right side, green little triangle)


(c) Fit a logistic regression model to the data, using $X_1$ and $X_2$ as predictors.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
# library(pander)
logit = glm(y ~ x1 + x2, family = "binomial")
summary(logit)
```


(d) Apply this model to training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
data = data.frame(x1 = x1, x2 = x2, y = y)
prob = predict(logit, data = data, type = "response")
pred = rep(0, 500); pred[prob > 0.5] = 1

# pred == 1, color = green triangle
plot(data[pred == 1, ]$x1, data[pred == 1, ]$x2, 
     col = (4 - 1), pch = (3 - 1), xlab = "X1", ylab = "X2")
# pred == 0, color = blue cross
points(data[pred == 0, ]$x1, data[pred == 0, ]$x2, 
       col = (4 - 0), pch = (3 - 0))
(result = table(data$y, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error) 
# green are mostly correct, the logistic prob boundary matters
threshold = seq(from=0.48,to = 0.56, by = 0.002)
index = 0
error = numeric(length(threshold))
for (i in threshold){
  pred = rep(0, 500); pred[prob > i] = 1
  result = table(data$y, pred)
  error[index] = sum(result[2:3])/sum(result)
  index = index +1
}
plot(threshold,error)
error[error==0]=NA # the reason I do it is because error is defaulted to be 0
threshold[which.min(error)]
# coincidentally this result is 0.502 and real distribution (0.484 vs 0.516)
```

- The decision boundary is linear. There are far more data classified as 1. 


(e) Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
logit.nl = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2), 
               family = "binomial")
summary(logit.nl)
```


(f) Apply this model to training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should obvioulsy be non-linear.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
prob = predict(logit.nl, data = data, type = "response")
pred = rep(0, 500); pred[prob > 0.5] = 1
# pred == 1, color = green triangle
plot(data[pred == 1, ]$x1, data[pred == 1, ]$x2, 
     col = (4 - 1), pch = (3 - 1), xlab = "X1", ylab = "X2")
# pred == 0, color = blue cross
points(data[pred == 0, ]$x1, data[pred == 0, ]$x2, 
       col = (4 - 0), pch = (3 - 0))
(result = table(data$y, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error) 

# nonlinear perfectly classified problems
threshold = seq(from=0.48,to = 0.56, by = 0.002)
index = 0
error = numeric(length(threshold))
for (i in threshold){
  pred = rep(0, 500); pred[prob > i] = 1
  result = table(data$y, pred)
  error[index] = sum(result[2:3])/sum(result)
  index = index +1
}
plot(threshold,error)
error[error==0]=NA # the reason I do it is because error is defaulted to be 0
threshold[which.min(error)]
# nonlinear perfectly classified problems
```

- Nonlinear logistic boundary is very close to real distribtion. 

(g) Fit a support vector classifier to the data with $X_1$ and $X_2$ as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
data$y = as.factor(data$y)
svm = svm(y ~ x1 + x2, data, kernel = "linear",cost = 0.01)
pred = rep(0, 500); pred = predict(svm, data = data)
plot(data[pred == 1, ]$x1, data[pred == 1, ]$x2, 
     col = (4 - 1), pch = (3 - 1), xlab = "X1", ylab = "X2")
points(data[pred == 0, ]$x1, data[pred == 0, ]$x2, 
       col = (4 - 0), pch = (3 - 0))

```

- This support vector classifier classifies all points to [y=1]


(h) Fit a SVM using a non-linear kernel to the data with $X_1$ and $X_2$ as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
data$y = as.factor(data$y)
svm.nl = svm(y ~ x1 + x2, data, kernel = "radial", gamma = 1)
pred = rep(0, 500); pred = predict(svm.nl, data = data)
plot(data[pred == 1, ]$x1, data[pred == 1, ]$x2, 
     col = (4 - 1), pch = (3 - 1), xlab = "X1", ylab = "X2")
points(data[pred == 0, ]$x1, data[pred == 0, ]$x2, 
       col = (4 - 0), pch = (3 - 0))
```

- Nonlinear vector support machine boundary is very close to real distribtion. 


(i) Comment on your results.

- SVM with non-linear kernel and nonlinear logistic regression with polynomial terms are equally good for finding non-linear decision boundaries. 
- SVM with linker kernel can perform equally bad as linear regression when the true boundary is not linear. 

\hfill

Q8. This problem involves the "OJ" data set which is part of the ISLR package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
set.seed(1)
library(ISLR)
size = nrow(OJ)
train = sample(size, 800)
data.train = OJ[train, ]; data.test = OJ[-train, ]
```

(b) Fit a support vector classifier to the training data using "cost" = 0.01, with "Purchase" as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm = svm(Purchase ~ ., data = data.train, kernel = "linear", cost = 0.01)
summary(svm)
```

- Comment: With relatively wider margin (cost = 0.01), support vector classifier takes 432 of the total 800 training data as support vectors. Out of these, 215 belong to level CH and the remaining 217 belong to level MM.

(c) What are the training and test error rates ?

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
pred = predict(svm, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

(d) Use the tune() function to select an optimal "cost". Consider values in the range 0.01 to 10.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
set.seed(3)
rm(svm)
tune.cost = tune(svm, Purchase ~ ., data = data.train, 
                 kernel = "linear", 
                 ranges = list(cost =c (0.01, 0.1, 1, 10, 100) ))
summary(tune.cost) #the optimal returned is cost = 1
```

(e) Compute the training and test error rates using this new value for "cost".

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm = svm(Purchase ~ ., kernel = "linear", 
          data = data.train, cost = tune.cost$best.parameter$cost)
pred = predict(svm, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for "gamma".

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm.radial = svm(Purchase ~ ., kernel = "radial", data = data.train)
summary(svm.radial)

pred = predict(svm.radial, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm.radial, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

Comment: Radial kernel with default gamma creates 379 support vectors, out of which, 188 classified as CH and 191 classified as MM. Radial kernel improved decreasing test MSE.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
set.seed(2)
rm(svm)
tune.cost = tune(svm, Purchase ~ ., data = data.train, kernel = "radial",
                 ranges = list(cost =c (0.01, 0.1, 1, 10, 100) ))
summary(tune.cost)
```
Comment: Still, cost = 1 is the optimal tuning parameter. 

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm.radial = svm(Purchase ~ ., kernel = "radial", data = data.train, 
                 cost = tune.cost$best.parameter$cost)
summary(svm.radial)

pred = predict(svm.radial, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm.radial, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

Comment: Tuning does not reduce train and test error rates as the default value of cost is 1, which is coincidentally the optimal value. 


(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set "degree" = 2.

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm.poly = svm(Purchase ~ ., kernel = "polynomial", 
               data = data.train, degree = 2)
summary(svm.poly)

pred = predict(svm.poly, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm.poly, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

Comment: Polynomial kernel with default gamma creates 454 support vectors, out of which, 224 belong to level CH and remaining 230 belong to level MM.


```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
set.seed(2)
rm(svm)
tune.cost = tune(svm, Purchase ~ ., data = data.train, 
                 kernel = "polynomial",degree = 2, 
                 ranges = list(cost =c (0.01, 0.1, 1, 10, 100) ))
summary(tune.cost)
```
Comment: Cost = 10 is the optimal tuning parameter. 

```{r, echo=T,message=FALSE,comment=NA,warning=FALSE}
svm.poly = svm(Purchase ~ ., data = data.train, kernel = "polynomial",
               degree = 2, cost = tune.cost$best.parameter$cost)
summary(svm.poly)

pred = predict(svm.poly, data.train)
(result = table(data.train$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The training error is ", error)

pred = predict(svm.poly, data.test)
(result = table(data.test$Purchase, pred))
error = sum(result[2:3])/sum(result)
cat("The testing error is ", error)
```

Comment:Tuning reduce train and test error rates.

(h) Overall, which approach seems to give the best results on this data ?

- Comment: Overall, SVM with radial basis kernel perform best result on this data. 
