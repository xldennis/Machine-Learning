---
title: "homework chapter 5 & 6"
author: "Dennis Liu"
date: "July 3th, 2015"
output:
  pdf_document:
    fig_height: 4.3
    fig_width: 8
    keep_tex: yes
  html_document:
    fig_height: 4
    fig_width: 8
geometry: margin=1in
setspace: onehalfspacing
fontsize: 12pt
---



# Statistical Learning - Chapter 5

## Exercises
### Conceptual
1. Using basic statistical properties of the variance, as well as single- variable calculus, derive (5.6). In other words, prove that $\alpha$ given by (5.6) does indeed minimize $Var(\alpha x + (1-\alpha)Y)$


Since we know the following:
$$Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y)$$
$$Var(cX) = c^2 Var(X)$$
$$Cov(cX,Y) = Cov(X,cY) = c Cov(X,Y)$$
We can have $$Var(\alpha X + (1 - \alpha)Y) = \alpha^2 Var(X) + (1 - \alpha)^2 Var(Y) + 2 \alpha (1 - \alpha) Cov(X, Y) \\ =  \sigma_X^2 \alpha^2 + \sigma_Y^2 (1 - \alpha)^2 + 2 \sigma_{XY} (-\alpha^2 + \alpha) \ \ (1)$$
Now we take derivative of (1) and find the critical point,
$$2 \sigma_X^2 \alpha + 2 \sigma_Y^2 (1 - \alpha) (-1) + 2 \sigma_{XY} (-2 \alpha + 1) = 0 \ \ (2)$$
And the solution of $\alpha$ to equation (2) is the following
$$\alpha = \frac {\sigma_Y^2 - \sigma_{XY}} {\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}$$

\hfill

3. We now review k-fold cross-validation.

(a) Explain how k-fold cross-validation is implemented.

- It is implemented by randomly splitting data (sample size: n) into k non-overlapping groups where each group is size of n/k. Each of these groups acts as a validation set and the remainder as a training set. Then on each round, we estimate MSE for this validation set and the overall test error is then estimated by averaging the k resulting MSE estimates.

(b) What are the advantages and disadvantages of k-fold cross-validation relative to:

i. The validation set approach? 

- Advantages: k-fold CV estimates are less biased (since we use more training data) and less variable (training/validation set split). Also, it is less likely to overestimate the test error rate for the model.  
- Disadvantages: k-fold CV is conceptually difficult to understand and it involves more computations and estimations. 

ii. LOOCV?

- Advantages: k-fold CV requires less time of model fitting (compared to n times). In LOOCV, we are averaging the outputs of n fitted models trained on an almost identical set of observations. These outputs can be highly correlated and the mean of highly correlated quantities has higher variance than less correlated ones.Therefore, the k-fold CV has lower variances. 
- Disadvantages: k-fold method can have larger bias than LOOCV since LOOCV has more data (n-1) in the training set. 

\hfill

### Applied
7. In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).

(a) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(ISLR)
library(pander)
data(Weekly)
set.seed(1)
attach(Weekly)
log.fit = glm(Direction~Lag1+Lag2, family=binomial)
pander(summary(log.fit)$coef)
```

(b) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
log.fit.1 = glm(Direction~Lag1+Lag2, data=Weekly[-1,], family=binomial)
pander(summary(log.fit)$coef)
```

(c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if P(Direction="Up"|Lag1, Lag2) > 0.5. Was this observation correctly classified?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
response = predict.glm(log.fit.1, Weekly[1,], type="response") > 0.5
cat("Was this observation correctly classified?  ", response)
```

(d) Write a for loop from i=1 to i=n,where n is the number of observations in the data set, that performs each of the following steps:

i. Fit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.
ii. Compute the posterior probability of the market moving up for the ith observation.
iii. Use the posterior probability for the ith observation in order to predict whether or not the market moves up.
iv. Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
n = length(Direction)
response = numeric(n)
prob = numeric(n)
error = numeric(n)
for (i in 1:n) {
  log.fit.loop = glm(Direction~Lag1+Lag2, 
                     data=Weekly[-i,], family=binomial)
  prob[i] = predict.glm(log.fit.loop, Weekly[i,], type="response")
  response[i] = ifelse(prob[i] > 0.5, c("Up"), c("Down")) 
  error[i] = (response[i] != Weekly[i, ]$Direction)
}
cat("The test error is  ", mean(error))
```

(e) Take the average of the n numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results.

- as shown above, the test error is arond 45% which is terrible. The chance to guess correctly is 50%. 

\hfill

8. We will now perform cross-validation on a simulated data set.
(a) Generate a simulated data set as follows: In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
```

- n is 100 and p is 2 (x and y)

(b) Create a scatterplot of X against Y . Comment on what you find.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
plot(x, y)
```

- it is an inverse-U shape suggesting fitting a quadratic model. 

(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares: Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y .

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(boot)
data = data.frame(x,y)
set.seed(1)
# i.
glm.fit.1 = glm(y~x)
cat("CV error of linear model is  ", cv.glm(data, glm.fit.1)$delta[1])
# ii.
glm.fit.2 = glm(y~poly(x,2))
cat("CV error of quadratic model is  ", cv.glm(data, glm.fit.2)$delta[1])
# iii.
glm.fit.3 = glm(y~poly(x,3))
cat("CV error of cubic model is  ", cv.glm(data, glm.fit.3)$delta[1])
# iv.
glm.fit.4 = glm(y~poly(x,4))
cat("CV error of quatric model is  ", cv.glm(data, glm.fit.4)$delta[1])
```

(d) Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
set.seed(3) # testing 
# i.
glm.fit.1 = glm(y~x)
cat("CV error of linear model is  ", cv.glm(data, glm.fit.1)$delta[1])
# ii.
glm.fit.2 = glm(y~poly(x,2))
cat("CV error of quadratic model is  ", cv.glm(data, glm.fit.2)$delta[1])
# iii.
glm.fit.3 = glm(y~poly(x,3))
cat("CV error of cubic model is  ", cv.glm(data, glm.fit.3)$delta[1])
# iv.
glm.fit.4 = glm(y~poly(x,4))
cat("CV error of quatric model is  ", cv.glm(data, glm.fit.4)$delta[1])
```

- It is exactly the same since LOOCV produces almost no bias (different test error due to training/testing split). And the quadratic model has the smallest cross-validation estimate of prediction error.  

(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

- the quadratic model has the smallest cross-validation estimate of prediction error. This is expected since it matches the true distribution of Y. 

(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
pander(glm.fit.1)
pander(glm.fit.2)
pander(glm.fit.3)
pander(glm.fit.4)
```

- linear term in linear model is not sigificant
- quadratic term in quadratic model is significant 
- cubic term in cubic model is not significant, quadratic term is significant though.
- quatric term in quatric model is not significant, quadratic term is significant though.


9. We will now consider the Boston housing data set, from the MASS library.
(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate $\hat \mu$.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(MASS)
attach(Boston)
summary(medv)
```

(b) Provide an estimate of the standard error of $\hat \mu$. Interpret this result. 

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
cat("The standard error is  ",   sd(medv)/sqrt(length(medv)))
```

(c) Now estimate the standard error of $\hat \mu$ using the bootstrap. How does this compare to your answer from (b)?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bstrap = boot(medv, boot.fn, 1000)
bstrap  # 0.4131285
```

- the standard error is very close to (b)

(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv).

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
cat("95 percent confidence interval: ", t.test(Boston$medv)$conf.int)
cat("95 percent Bootstrapping confidence interval: ", 
    c(bstrap$t0 - 2*0.4119, bstrap$t0 + 2*0.4119))
```

(e) Based on this data set, provide an estimate, $\hat \mu_{med}$, for the median value of medv in the population.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
cat("the point estimate for median value of medv in the population:  "
    , median(medv))
```

(f) We now would like to estimate the standard error of $\hat \mu_{med}$.Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
boot.fn = function(data, index) return(median(data[index]))
bstrap = boot(medv, boot.fn, 1000)
bstrap # 0.3866274
```

- The standard error is 0.3866274 and the median value is 21.2. There is little variation of median value. 

(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity $\hat \mu_{0.1}$. (You can use the quantile() function.)

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
cat("the point estimate for the tenth percentile 
    of medv in the population:  ", quantile(medv, c(0.1)))
```

(h) Use the bootstrap to estimate the standard error of $\hat \mu_{0.1}$. Comment on your findings.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
boot.fn = function(data, index) return(quantile(data[index], c(0.1)))
bstrap = boot(medv, boot.fn, 1000)
bstrap # 0.5118644
```

- Tenth-percentile of medv is 12.75 with standard error of 0.511. Small standard error relative to the point estimate of tenth-percentile value.


# Statistical Learning - Chapter 6

## Exercises
### Conceptual

1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:


(a) Which of the three models with k predictors has the smallest training RSS?

- Best subset selection has the smallest training RSS because the other two methods determine models on which predictors they pick first as they iterate to the kth model. 

(b) Which of the three models with k predictors has the smallest test RSS?

- Best subset selection may have the smallest test RSS because it considers more models then the other methods. However, the other models can be lucky to predict a model that fits the test data. But subset, though its computational expense, is guaranteed to find a model with smallest test RSS.

(c) True or False:

i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection. 

- True. The model with (k+1) predictors is obtained by adding one additional predictors in the model with k predictors.

ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection.

- True. The model with k predictors is obtained by removing one predictor from the model with (k+1) predictors.

iii. The predictors in the k-variable model identified by back- ward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.

- False, they have different algorithm in selecting variables into the model. 

iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.

- False, they have different algorithm in selecting variables into the model.

v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.

- False, not necessarily to be the same. 

\hfill

4. Suppose we estimate the regression coefficients in a linear regression model by minimizing 
$$\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p\beta_j^2$$
for a particular value of $\lambda$. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.


(a) As we increase $\lambda$ from 0, the training RSS will

- Steadily increase. As we increase $\lambda$ from 0, we are restricting the $\beta_j$ coefficients more and more and the coefficients will deviate from their least squares estimates more and more. This will steadily increase the training RSS. 

(b) Repeat (a) for test RSS.

- Decrease initially, and then eventually start increasing in a U shape. As we increase $\lambda$ from 0, we are restricting the $\beta_j$ coefficients more and more, and the model is becoming less and less flexible which decreases substantial amount of the variance and increases a little bit of bias, and which decrease the test RSS first. But then with less decrease of variance, the large bias due to overfitting the data may increase the test RSS.

(c) Repeat (a) for variance.

- Steadily decrease. As we increase $\lambda$ from 0, we are restricting the $\beta_j$ coefficients more and more, and the model is becoming less and less flexible which decreases substantial amount of the variance.

(d) Repeat (a) for (squared) bias.

Steadily increase. As we increase $\lambda$ from 0, we are restricting the $\beta_j$ coefficients more and more, and the model is becoming less and less flexible which will decrease the variance and in return increase the bias.

(e) Repeat (a) for the irreducible error.

Remain constant. By definition, the irreducible error is independant of the model, and consequently independant of the value of $\lambda$.

\hfill

5. It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give different coefficient values to correlated variables. We will now explore this property in a very simple setting.

Suppose that $n = 2$, $p = 2$, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore, suppose that $y_1 + y_2 = 0$ and $x_{11} + x_{21} = 0$ and $x_{12} + x_{22} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero : $\hat{\beta}_0 = 0$.

(a) Write out the ridge regression optimization problem in this setting.

According to this setting ($x_{11} = x_{12} = x_1$ and $x_{21} = x_{22} = x_2$), the ridge regression problem seeks to minimize 
$$\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p\beta_j^2 \\ =(y_1 - \hat{\beta}_1x_1 - \hat{\beta}_2x_1)^2 + (y_1 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2 + \lambda(\hat{\beta}_1^2 + \hat{\beta}_2^2)$$

(b) Argue that in this setting, the ridge coefficient estimates satisfy $\hat{\beta}_1 = \hat{\beta}_2$.

By taking the derivatives of the above expression with respect to $\hat{\beta}_1$, we obtain the following:
$$\hat{\beta}_1(x_1^2 + x_2^2 + \lambda) + \hat{\beta}_2(x_1^2 + x_2^2) = y_1x_1 + y_2x_2$$
By taking the derivatives of the above expression with respect to $\hat{\beta}_2$, we obtain the following:
$$\hat{\beta}_1(x_1^2 + x_2^2) + \hat{\beta}_2(x_1^2 + x_2^2 + \lambda) = y_1x_1 + y_2x_2$$
By substracting the two expressions above we get $\hat{\beta}_1 = \hat{\beta}_2$.

(c) Write out the lasso optimization problem in this setting.

Since $\hat{\beta}_0 = 0$, the lasso optimization problem seeks to minimize the following: 
$$(y_1 - \hat{\beta}_1x_1 - \hat{\beta}_2x_1)^2 + (y_2 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2 + \lambda(|\hat{\beta}_1| + |\hat{\beta}_2|)$$

(d) Argue that in this setting, the lasso coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$ are not unique; in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.

Since it is equivalent to find $\beta's$ in the following setting:
$$(y_1 - \hat{\beta}1x_1 - \hat{\beta}_2x_1)^2 + (y_2 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2\text{ given }|\hat{\beta}_1| + |\hat{\beta}_2|\le s$$ Geometrically the lasso constraint take the form of a diamond centered at the origin of the plane $(\hat{\beta}_1,\hat{\beta}_2)$ which intersects the axes at a distance $s$ from the origin. 

By looking at the Least Square side, we have to minimize the exression $$(y_1 - (\hat{\beta}_1 + \hat{\beta}_2)x_1)^2$$ 
This optimization problem has a simple solution : $\hat{\beta}_1 + \hat{\beta}_2 = y_1/x_1$. 
Geometrically, this is a line parallel to sides of the diamond of the constraints. Now, solutions to the lasso optimization problem are contours of the function $(y_1 - (\hat{\beta}_1 + \hat{\beta}_2)x_1)^2$ that intersects the diamond of the constraints. 

So, the entire side $\hat{\beta}_1 + \hat{\beta}_2 = s$  and $\hat{\beta}_1 + \hat{\beta}_2 = -s$ are the possible solutions to the lasso optimization problem. Thus, the lasso optimization problem has a whole set of solutions instead of a unique one:
$${(\hat{\beta}_1,\hat{\beta}_2) : \hat{\beta}_1 + \hat{\beta}_2 = s\text{ with }\hat{\beta}_1,\hat{\beta}_2\ge 0\text{ and }\hat{\beta}_1 + \hat{\beta}_2 = -s\text{ with }\hat{\beta}_1,\hat{\beta}_2\le 0}$$

\hfill

7. We will now derive the Bayesian connection to the lasso and ridge regression discussed in Section 6.2.2.

a. The likelihood for OLS data is:

$$\begin{aligned} \mathcal{L}(\theta \mid \beta) \\ &= p(\beta \mid \theta) \ = p(\beta_1 \mid \theta) \times \cdots \times p(\beta_n \mid \theta) \\ &= \prod_{i = 1}^{n} \frac{ 1 }{ \sigma \sqrt{2\pi} } \exp \left(- \frac{ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) }{ 2\sigma^2 } \right) \\ &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 \right) \end{aligned}$$

b. prior $\beta's$ are independent and identically distributed with double exponential with mean 0 and common scale parameter $b$, i.e. $p(\beta) = \frac{1}{2b}\exp(- \lvert \beta \rvert / b)$.

Since the posterior probability relates to the prior probability by the following:
$$f(\beta \mid X, Y) \propto f(Y \mid X, \beta) p(\beta \mid X) = f(Y \mid X, \beta) p(\beta) $$
We can plug (a) in the above equation and we have:
$$\begin{aligned} f(\beta \mid X, Y) \\ &= f(Y \mid X, \beta)p(\beta)  \\ &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 \right) \left( \frac{ 1 }{ 2b } \exp(- \lvert \beta \rvert / b) \right) \\ &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \left( \frac{ 1 }{ 2b } \right) \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 - \frac{ \lvert \beta \rvert }{ b } \right) \end{aligned}$$

c. Argue that the lasso estimate is the mode for $\beta$ under this posterior distribution.

First take logrithm of both sides since it is harmonic:

$$ \begin{aligned} \log (f(Y \mid X, \beta)p(\beta)) &= \log \left[ \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \left( \frac{ 1 }{ 2b } \right) \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 - \frac{ \lvert \beta \rvert }{ b } \right) \right] \\ &= -\frac{ 1 }{ 2 } \log(2 \sigma^2 \pi) - log(2b) - \left( \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \frac{ \lvert \beta \rvert }{ b } \right) \end{aligned}$$

Since only the last part is to do with $\beta$, we want to minimize the last term: $$ \begin{aligned} \arg\min_\beta \, \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \frac{ \lvert \beta \rvert }{ b } \end{aligned}$$

By letting $\sigma^2/ b = \lambda$, we end up with:
$$ \begin{aligned} &=  \frac{ 1 }{ 2\sigma^2 }  \arg\min_\beta \,  \left( \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \lambda \sum_{i = 1}^{p} \beta_i^2 \right) \end{aligned} $$

Since this is a normal distribution with mean zero and variance $c$,, i.e., $\sigma^2/ b = \lambda$ then it follows that the posterior mode for $\beta$ is the lasso solution. 


d. prior $\beta's$ are independent and identically distributed with normal distribution with mean 0 and common scale parameter $c$, i.e. $p(\beta) = \frac{1}{2b}\exp(- \lvert \beta \rvert / c)$.

Similarly, we can estimate posterior distribution of $\beta$:

$$ \begin{aligned} f(Y \mid X, \beta)p(\beta) \\ &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 \right) \left( \frac{ 1 }{ \sqrt{ 2c\pi } } \right)^p \exp \left( - \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right) \\ &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \left( \frac{ 1 }{ \sqrt{ 2c\pi } } \right)^p \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 - \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right) \end{aligned}$$

Ps-Marginal-Probability: $$ p(\beta) = \prod_{i = 1}^{p} p(\beta_i) = \prod_{i = 1}^{p} \frac{ 1 }{ \sqrt{ 2c\pi } } \exp \left( - \frac{ \beta_i^2 }{ 2c } \right) = \left( \frac{ 1 }{ \sqrt{ 2c\pi } } \right)^p \exp \left( - \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right) $$


e. Argue that the ridge regression estimate is both the mode and the mean for $\beta$ under this posterior distribution.

Same process as in part (c), take logrithm of both sides since it is harmonic:

$$\begin{aligned} \log f(Y \mid X, \beta)p(\beta) &= \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \left( \frac{ 1 }{ \sqrt{ 2c\pi } } \right)^p \exp \left( - \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 - \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right) \\ &= \log \left[ \left( \frac{ 1 }{ \sigma \sqrt{2\pi} } \right)^n \left( \frac{ 1 }{ \sqrt{ 2c\pi } } \right)^p \right] - \left( \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right) \end{aligned}$$

Since only the last part is to do with $\beta$, we want to minimize the last term: $$\begin{aligned} \arg\min_\beta \, \left( \frac{ 1 }{ 2\sigma^2 } \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \frac{ 1 }{ 2c } \sum_{i = 1}^{p} \beta_i^2 \right)\end{aligned}$$

By letting $\sigma^2/ c = \lambda$, we end up with:
$$ \begin{aligned} &= \arg\min_\beta \, \left( \frac{ 1 }{ 2\sigma^2 } \right) \left( \sum_{i = 1}^{n} \left[ Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij}) \right]^2 + \lambda \sum_{i = 1}^{p} \beta_i^2 \right) \end{aligned}$$

Since this is a normal distribution with mean zero and variance $c = \sigma^2/\lambda$, then it follows that the posterior mode for $\beta$—that posterior is, the most likely value for $\beta$, given the data is given by the ridge mode regression solution.


### Applied
9. In this exercise, we will predict the number of applications received using the other variables in the College data set.


(a) Split the data set into a training and a test set.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
set.seed(2)
samplesize = nrow(College)
train = sample(1:samplesize, samplesize/2)
test = -train
College.train = College[train, ]
College.test = College[test, ]
```

(b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
fit.lm = lm(Apps ~ ., data = College.train) #numb of applications
pred.lm = predict(fit.lm, College.test)
error = mean((pred.lm - College.test$Apps)^2)
cat("The test error of linear regression model is  ",  error)
```

(c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(glmnet)
train.matrix = model.matrix(Apps ~ ., data = College.train)
# design matrix of coefficient contrast. anova in regression way
test.matrix = model.matrix(Apps ~ ., data = College.test)
# glmnet() function can only take quantitative input and std. variable
grid = 10 ^ seq(5, -2, length = 100)
cv.ridge = cv.glmnet(train.matrix, College.train$Apps,
                     alpha = 0, lambda = grid, thresh = 1e-12)
plot(cv.ridge)
bestlamb.ridge = cv.ridge$lambda.min
pred.ridge = predict(cv.ridge$glmnet.fit, s = bestlamb.ridge, test.matrix)
# same to fit a ridge model
error = mean((pred.ridge - College.test$Apps)^2)
cat("The test error of Ridge regression model is  ",  error)
```

(d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
# grid = 10 ^ seq(5, -2, length = 100)
cv.lasso = cv.glmnet(train.matrix, College.train$Apps,
                     alpha = 1, lambda = grid, thresh = 1e-12)
plot(cv.lasso)
bestlamb.lasso = cv.lasso$lambda.min
pred.lasso = predict(cv.lasso$glmnet.fit, s = bestlamb.lasso, test.matrix)
error = mean((pred.lasso - College.test$Apps)^2)
cat("The test error of Lasso regression model is  ",  error)
#coefficient of lasso regression model
predict(cv.lasso$glmnet.fit, s = bestlamb.lasso, type = "coefficients")
cat("Every predictor except enroll has non-zero coefficient")
```

(e) Fit a PCR model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(pls)
fit.pcr = pcr(Apps ~ ., data = College.train, 
              scale = TRUE, validation = "CV")
validationplot(fit.pcr, val.type = "MSEP")
cat("looks like 10 is a good choice but is it precisely good ?")
pred.pcr = predict(fit.pcr, College.test, ncomp = 10)
error = mean((pred.pcr - College.test$Apps)^2)
cat("The test error of Principal Component Regression is  ",  error)
```

(f) Fit a PLS model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(pls)
fit.plsr = plsr(Apps ~ ., data = College.train, 
              scale = TRUE, validation = "CV")
validationplot(fit.plsr, val.type = "MSEP")
cat("looks like 10 is a good choice but is it precisely good ?")
# do we use that 1.standard rule?
pred.plsr = predict(fit.plsr, College.test, ncomp = 10)
error = mean((pred.plsr - College.test$Apps)^2)
cat("The test error of Principal Component Regression is  ",  error)
```

(g) Comment on the results obtained. How accurately can we predict the number of college applications received ? Is there much difference among the test errors resulting from these five approaches ?

Compare the test $R^2$ for all models.
```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
test.avg = mean(College.test$Apps)
lm.r2 = 1 - mean((pred.lm - College.test$Apps)^2) / mean((College.test$Apps - test.avg)^2)
ridge.r2 = 1 - mean((pred.ridge - College.test$Apps)^2) / mean((College.test$Apps - test.avg)^2)
lasso.r2 = 1 - mean((pred.lasso - College.test$Apps)^2) / mean((College.test$Apps - test.avg)^2)
pcr.r2 = 1 - mean((pred.pcr - College.test$Apps)^2) / mean((College.test$Apps - test.avg)^2)
plsr.r2 = 1 - mean((pred.plsr - College.test$Apps)^2) / mean((College.test$Apps - test.avg)^2)
rbind(lm.r2,ridge.r2,lasso.r2,pcr.r2,plsr.r2)
```

- all seem to perform well except Principal Component Regression. Lasso is more accurate than ridge regression. 

\hfill 

10. We have seen that as a number of features used in a model increases, the training error will ncessarily decrease, but the test error may not. We will now explore this in a simulated data set.

(a) Generate a data set with $p = 20$ features, $n = 1000$ observations, and an associated quantitative response vector generated according to the model $$Y = X\beta + \epsilon$$ where $\beta$ has some elements that are exactly equal to zero.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
set.seed(1)
x = matrix(rnorm(1000 * 20), 1000, 20)
b = rnorm(20); b[3] = b[9] = b[13] = b[15] = b[20] = 0
epsilon =  rnorm(1000)
y = x %*% b + epsilon
```

(b) Split your data set into a training set containing 100 observations and a test set containing 900 observations.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
train = sample(seq(1000), 100, replace = FALSE)
test = -train
x.train = x[train, ]; x.test = x[test, ]
y.train = y[train]; y.test = y[test]
```

(c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
data.train = data.frame(y = y.train, x = x.train)
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.train, nvmax = 20)
reg.summary = summary(regfit.full) 
# see which predictors are selected at each folding
train.matrix = model.matrix(y ~ ., data = data.train, nvmax = 20)
# fit training data least square regression
val.errors.train = rep(NA, 20) #or numeric(20)
for (i in 1:20) {
    coef = coef(regfit.full, id = i)
    pred = train.matrix[, names(coef)] %*% coef
    val.errors.train[i] = mean((y.train - pred)^2)
}
plot(val.errors.train, xlab = "Number of predictors", 
     ylab = "Training MSE",pch = 20, type = "b", col = "green")
cat("The model with smallest training MSE has ", 
    which.min(val.errors.train), " predictors")
```

(d) Plot the test MSE associated with the best model of each size.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
data.test = data.frame(y = y.test, x = x.test)
test.matrix = model.matrix(y ~ ., data = data.test, nvmax = 20)
val.errors.test = rep(NA, 20) #or numeric(20)
for (i in 1:20) {
    coef = coef(regfit.full, id = i)
    pred = test.matrix[, names(coef)] %*% coef
    val.errors.test[i] = mean((y.test - pred)^2)
}
plot(val.errors.test, xlab = "Number of predictors", 
     ylab = "Testing MSE",pch = 20, type = "b", col = "blue")
cat("The model with smallest testing MSE has ", 
    which.min(val.errors.test), " predictors")
```

(e) For which model size does the test set MSE take on its minimum value ? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test MSE is minimized for an intermediate model size.

- It is very clear that the training MSE will decrease as more predictors are included in the model. However, the testing MSE does not show the same pattern. 

(f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data ? Comment on the coefficient values.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
coef(regfit.full, which.min(val.errors.test))
```

- Most true zero-coefficients are not in the optimal model. X.20 should be zero while X.6 should not. 

(g) Create a plot displaying $\sqrt{\sum_{j=1}^p(\beta_j - \hat{\beta}_j^r)^2}$ for a range of values of $r$ where $\hat{\beta}_j^r$ is the jth coefficient estimate for the best model containing $r$ coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d) ?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
val.errors = rep(NA, 20)
x.column = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:20) {
    coef = coef(regfit.full, id = i)
    val.errors[i] = sqrt(sum((b[x.column %in% names(coef)] 
                              - coef[names(coef) %in% x.column])^2) 
                         + sum(b[!(x.column %in% names(coef))]-0)^2)
    # b starts with beta_1 and it is the true parameter
    # coef starts with beta_0 for each best subset model
    # a %in% b check if a is in b
}
plot(val.errors, xlab = "Number of coefficients", 
     ylab = "Error between estimated and true coefficients", 
     pch = 20, type = "b", col = "orange")
cat("The model with smallest estimate error has ", 
    which.min(val.errors), " predictors")
```

- The better fit of true coefficients does not mean lower testing MSE.


